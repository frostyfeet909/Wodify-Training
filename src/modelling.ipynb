{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn import svm\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = Path().cwd().parent / \"data\" / \"workouts.pkl\"\n",
    "data = pd.read_pickle(data_file)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "\n",
    "x = df.drop(['attendance', 'overflow'], axis = 1)\n",
    "y = df.attendance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = []\n",
    "columns_to_drop = []\n",
    "\n",
    "for i in x.columns:\n",
    "    if x[i].nunique()<15 and x[i].dtype==\"object\":\n",
    "        categorical_columns.append(i)\n",
    "    elif x[i].nunique()>=15 and x[i].dtype==\"object\":\n",
    "        columns_to_drop.append(i)\n",
    "\n",
    "print(categorical_columns)\n",
    "print(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', onehot_encoder, categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other columns unchanged\n",
    ")\n",
    "\n",
    "x_transformed = preprocessor.fit_transform(x)\n",
    "new_column_names = preprocessor.get_feature_names_out()\n",
    "new_column_names = [name.split('__')[-1] for name in new_column_names]\n",
    "x = pd.DataFrame(x_transformed, columns=new_column_names) # type: ignore - False positive for pandas interface.\n",
    "x = x.astype({col: 'int' for col in x.columns if x[col].dtype == 'float64' and x[col].apply(float.is_integer).all()})\n",
    "\n",
    "x.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "# print(y_test)\n",
    "# scaler = StandardScaler()\n",
    "# x_train = pd.DataFrame(scaler.fit_transform(x_train), columns = x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the comparison dictionary with additional metrics\n",
    "comparison_dict = {\n",
    "    'model': [],\n",
    "    'params': [],\n",
    "    'R^2': [],\n",
    "    'MAE': [],\n",
    "    'MSE': [],\n",
    "    'RMSE': [],\n",
    "    'MAPE': [],\n",
    "    'Explained Variance': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "\n",
    "params={'criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': list(int(i) for i in np.linspace(5, 55, 26)) + [None]}\n",
    "\n",
    "for criterion in params['criterion']:\n",
    "    for max_features in params['max_features']:\n",
    "        for max_depth in params['max_depth']:\n",
    "            model_params = (criterion, max_features, max_depth)\n",
    "            model = RandomForestRegressor(criterion = criterion,\n",
    "                                          max_features = max_features, max_depth = max_depth, random_state = 1)\n",
    "            model.fit(x_train, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(x_test)\n",
    "            \n",
    "            # Compute scores\n",
    "            r2_score = model.score(x_test, y_test)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            explained_var = explained_variance_score(y_test, y_pred)\n",
    "            \n",
    "            # Record the results\n",
    "            comparison_dict['model'].append('random_forest_regressor')\n",
    "            comparison_dict['params'].append(model_params)\n",
    "            comparison_dict['R^2'].append(r2_score)\n",
    "            comparison_dict['MAE'].append(mae)\n",
    "            comparison_dict['MSE'].append(mse)\n",
    "            comparison_dict['RMSE'].append(rmse)\n",
    "            comparison_dict['MAPE'].append(mape)\n",
    "            comparison_dict['Explained Variance'].append(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "params={'gamma': np.logspace(-4, -1, 10),\n",
    "        'C': np.logspace(-2, 1, 10),\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid']} \n",
    "\n",
    "for gamma in params['gamma']:\n",
    "    for c in params['C']:\n",
    "        for kernel in params['kernel']:\n",
    "            model_params = (gamma, c, kernel)\n",
    "            model = svm.SVR(gamma = gamma, C = c, kernel = kernel)\n",
    "            model.fit(x_train, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(x_test)\n",
    "            \n",
    "            # Compute scores\n",
    "            r2_score = model.score(x_test, y_test)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            explained_var = explained_variance_score(y_test, y_pred)\n",
    "            \n",
    "            # Record the results\n",
    "            comparison_dict['model'].append('svr_regressor')\n",
    "            comparison_dict['params'].append(model_params)\n",
    "            comparison_dict['R^2'].append(r2_score)\n",
    "            comparison_dict['MAE'].append(mae)\n",
    "            comparison_dict['MSE'].append(mse)\n",
    "            comparison_dict['RMSE'].append(rmse)\n",
    "            comparison_dict['MAPE'].append(mape)\n",
    "            comparison_dict['Explained Variance'].append(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "\n",
    "params={'hidden_layer_sizes': [(80,20,40,5), (75,30,50,10,3)], \n",
    "        'activation': ['identity', 'relu','logistic', 'tanh',], \n",
    "        'solver': ['lbfgs','sgd', 'adam'], \n",
    "        'alpha': np.logspace(-4,1,20)} \n",
    "\n",
    "for hidden_layer_sizes in params['hidden_layer_sizes']:\n",
    "    for activation in params['activation']:\n",
    "        for solver in params['solver']:\n",
    "            for alpha in params['alpha']:\n",
    "                model_params = (hidden_layer_sizes, activation, solver, alpha )\n",
    "                model = MLPRegressor(hidden_layer_sizes = hidden_layer_sizes,\n",
    "                                      activation = activation, solver = solver, alpha = alpha, random_state = 1)\n",
    "                model.fit(x_train, y_train)\n",
    "\n",
    "            # Predictions\n",
    "            y_pred = model.predict(x_test)\n",
    "            \n",
    "            # Compute scores\n",
    "            r2_score = model.score(x_test, y_test)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            explained_var = explained_variance_score(y_test, y_pred)\n",
    "            \n",
    "            # Record the results\n",
    "            comparison_dict['model'].append('mlp_regressor')\n",
    "            comparison_dict['params'].append(model_params)\n",
    "            comparison_dict['R^2'].append(r2_score)\n",
    "            comparison_dict['MAE'].append(mae)\n",
    "            comparison_dict['MSE'].append(mse)\n",
    "            comparison_dict['RMSE'].append(rmse)\n",
    "            comparison_dict['MAPE'].append(mape)\n",
    "            comparison_dict['Explained Variance'].append(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "\n",
    "params={'fit_intercept': [True, False]}\n",
    "\n",
    "for fit_intercept in params['fit_intercept']:\n",
    "    model_params = (fit_intercept)\n",
    "    model = Lasso(fit_intercept = fit_intercept)\n",
    "    model.fit(x_train, y_train)\n",
    "            \n",
    "    # Predictions\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Compute scores\n",
    "    r2_score = model.score(x_test, y_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    explained_var = explained_variance_score(y_test, y_pred)\n",
    "    \n",
    "    # Record the results\n",
    "    comparison_dict['model'].append('lasso')\n",
    "    comparison_dict['params'].append(model_params)\n",
    "    comparison_dict['R^2'].append(r2_score)\n",
    "    comparison_dict['MAE'].append(mae)\n",
    "    comparison_dict['MSE'].append(mse)\n",
    "    comparison_dict['RMSE'].append(rmse)\n",
    "    comparison_dict['MAPE'].append(mape)\n",
    "    comparison_dict['Explained Variance'].append(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet Regression\n",
    "\n",
    "params={'fit_intercept': [True, False]}\n",
    "\n",
    "for fit_intercept in params['fit_intercept']:\n",
    "    model_params = (fit_intercept)\n",
    "    model = ElasticNet(fit_intercept = fit_intercept)\n",
    "    model.fit(x_train, y_train)\n",
    "            \n",
    "    # Predictions\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Compute scores\n",
    "    r2_score = model.score(x_test, y_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    explained_var = explained_variance_score(y_test, y_pred)\n",
    "    \n",
    "    # Record the results\n",
    "    comparison_dict['model'].append('elastic_net')\n",
    "    comparison_dict['params'].append(model_params)\n",
    "    comparison_dict['R^2'].append(r2_score)\n",
    "    comparison_dict['MAE'].append(mae)\n",
    "    comparison_dict['MSE'].append(mse)\n",
    "    comparison_dict['RMSE'].append(rmse)\n",
    "    comparison_dict['MAPE'].append(mape)\n",
    "    comparison_dict['Explained Variance'].append(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "params={'fit_intercept': [True, False]}\n",
    "\n",
    "for fit_intercept in params['fit_intercept']:\n",
    "    model_params = (fit_intercept)\n",
    "    model = LinearRegression(fit_intercept = fit_intercept)\n",
    "    model.fit(x_train, y_train)\n",
    "            \n",
    "    # Predictions\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Compute scores\n",
    "    r2_score = model.score(x_test, y_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    explained_var = explained_variance_score(y_test, y_pred)\n",
    "    \n",
    "    # Record the results\n",
    "    comparison_dict['model'].append('linear_regression')\n",
    "    comparison_dict['params'].append(model_params)\n",
    "    comparison_dict['R^2'].append(r2_score)\n",
    "    comparison_dict['MAE'].append(mae)\n",
    "    comparison_dict['MSE'].append(mse)\n",
    "    comparison_dict['RMSE'].append(rmse)\n",
    "    comparison_dict['MAPE'].append(mape)\n",
    "    comparison_dict['Explained Variance'].append(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients from the model\n",
    "coefficients = model.coef_\n",
    "\n",
    "# Create a DataFrame to hold feature names and their coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': x.columns,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by the absolute value of coefficients\n",
    "coef_df['Absolute Coefficient'] = coef_df['Coefficient'].abs()\n",
    "coef_df = coef_df.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(coef_df['Feature'], coef_df['Coefficient'], color='skyblue')\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance in Linear Regression')\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients from the model\n",
    "coefficients = model.coef_\n",
    "\n",
    "# Extract only the coach-related coefficients\n",
    "coach_columns = [col for col in x.columns if col.startswith('coach_')]\n",
    "coach_coefficients = [coefficients[x.columns.get_loc(col)] for col in coach_columns]\n",
    "\n",
    "# Create a DataFrame to hold coach names and their coefficients\n",
    "coach_coef_df = pd.DataFrame({\n",
    "    'Coach': [col.replace('coach_', '') for col in coach_columns],\n",
    "    'Coefficient': coach_coefficients\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by the absolute value of coefficients\n",
    "coach_coef_df['Absolute Coefficient'] = coach_coef_df['Coefficient'].abs()\n",
    "coach_coef_df = coach_coef_df.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "\n",
    "print(coach_coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coefficients for coaches\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(coach_coef_df['Coach'], coach_coef_df['Coefficient'], color='skyblue')\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Coach')\n",
    "plt.title('Ranking of Coaches by Feature Importance in Linear Regression')\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients from the model\n",
    "coefficients = model.coef_\n",
    "\n",
    "# Extract only the time-related coefficients\n",
    "time_columns = [col for col in x.columns if col.startswith('time_')]\n",
    "time_coefficients = [coefficients[x.columns.get_loc(col)] for col in time_columns]\n",
    "\n",
    "# Create a DataFrame to hold time names and their coefficients\n",
    "time_coef_df = pd.DataFrame({\n",
    "    'Time': [col.replace('time_', '') for col in time_columns],\n",
    "    'Coefficient': time_coefficients\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by the absolute value of coefficients\n",
    "time_coef_df['Absolute Coefficient'] = time_coef_df['Coefficient'].abs()\n",
    "time_coef_df = time_coef_df.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "\n",
    "print(time_coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coefficients for coaches\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(time_coef_df['Time'], time_coef_df['Coefficient'], color='skyblue')\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Time')\n",
    "plt.title('Ranking of Times by Feature Importance in Linear Regression')\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(comparison_dict)\n",
    "\n",
    "# Example: Plot R^2 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='model', y='R^2', data=comparison_df, palette='viridis')\n",
    "plt.title('R^2 Scores Across Different Models')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('R^2 Score')\n",
    "plt.show()\n",
    "\n",
    "# Example: Plot MAE\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='model', y='MAE', data=comparison_df, palette='magma')\n",
    "plt.title('MAE Across Different Models')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.show()\n",
    "\n",
    "# Example: Plot RMSE\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='model', y='RMSE', data=comparison_df, palette='coolwarm')\n",
    "plt.title('RMSE Across Different Models')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Root Mean Squared Error (RMSE)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the scores for comparison (optional)\n",
    "comparison_df['normalized_r2'] = comparison_df['R^2'] / comparison_df['R^2'].max()\n",
    "comparison_df['normalized_mae'] = comparison_df['MAE'].min() / comparison_df['MAE']\n",
    "comparison_df['normalized_rmse'] = comparison_df['RMSE'].min() / comparison_df['RMSE']\n",
    "\n",
    "# Compute a composite score (higher is better)\n",
    "comparison_df['composite_score'] = (comparison_df['normalized_r2'] +\n",
    "                                    comparison_df['normalized_mae'] +\n",
    "                                    comparison_df['normalized_rmse']) / 3\n",
    "\n",
    "# Find the index of the best model based on the composite score\n",
    "best_model_index = comparison_df['composite_score'].idxmax()\n",
    "\n",
    "# Extract the details of the best model\n",
    "best_model = comparison_df.loc[best_model_index]\n",
    "\n",
    "# Display the best model's parameters and metrics\n",
    "print(\"Best Model based on Composite Score:\")\n",
    "print(f\"Model: {best_model['model']}\")\n",
    "print(f\"Parameters: {best_model['params']}\")\n",
    "print(f\"Composite Score: {best_model['composite_score']}\")\n",
    "print(f\"R^2 Score: {best_model['R^2']}\")\n",
    "print(f\"MAE: {best_model['MAE']}\")\n",
    "print(f\"MSE: {best_model['MSE']}\")\n",
    "print(f\"RMSE: {best_model['RMSE']}\")\n",
    "print(f\"MAPE: {best_model['MAPE']}\")\n",
    "print(f\"Explained Variance: {best_model['Explained Variance']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by the composite score\n",
    "sorted_comparison_df = comparison_df.sort_values(by='composite_score', ascending=False)\n",
    "\n",
    "# Plot the top models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='model', y='composite_score', data=sorted_comparison_df.head(10), palette='viridis')\n",
    "plt.title('Top 10 Models by Composite Score')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Composite Score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
